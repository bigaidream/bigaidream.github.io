<!DOCTYPE html>
<html lang="en">
    <head>
        <title>Big AI Dream | Jie Fu</title>
        <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
        <meta name="author" content="Jie Fu">
        <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
        
        <meta name="description" content="A theme for faculty profile page" />
        <meta name="keywords" content="faculty profile, theme,css, html, jquery, transition, transform, 3d, css3" />

        <link rel="shortcut icon" href="../favicon.ico">

        <!--CSS styles-->
        <link rel="stylesheet" href="css/bootstrap.css">
        <link rel="stylesheet" href="css/font-awesome.min.css">  
        <link rel="stylesheet" href="css/perfect-scrollbar-0.4.5.min.css">
        <link rel="stylesheet" href="css/magnific-popup.css">
        <link rel="stylesheet" href="css/style.css">
        <link id="theme-style" rel="stylesheet" href="css/styles/default.css">

        
        <!--/CSS styles-->
        <!--Javascript files-->
        <script type="text/javascript" src="js/jquery-1.11.3.min.js"></script>
        <script type="text/javascript" src="js/TweenMax.min.js"></script>
        <script type="text/javascript" src="js/jquery.touchSwipe.min.js"></script>
        <script type="text/javascript" src="js/jquery.carouFredSel-6.2.1-packed.js"></script>
        
        <script type="text/javascript" src="js/modernizr.custom.63321.js"></script>
        <script type="text/javascript" src="js/jquery.dropdownit.js"></script>

        <script type="text/javascript" src="js/ScrollToPlugin.min.js"></script>

        <script type="text/javascript" src="js/bootstrap.min.js"></script>

        <script type="text/javascript" src="js/jquery.mixitup.min.js"></script>

        <script type="text/javascript" src="js/masonry.min.js"></script>

        <script type="text/javascript" src="js/perfect-scrollbar-0.4.5.with-mousewheel.min.js"></script>
        <script type="text/javascript" src="js/jquery.nicescroll.min.js"></script>
        
        <script type="text/javascript" src="js/magnific-popup.js"></script>
        <script type="text/javascript" src="js/custom.js"></script>

        <link rel="icon" type="image/png" href="img/favicon.png">

        <!--/Javascript files-->

    </head>
    <body>

        <div id="wrapper">
            <a href="#sidebar" class="mobilemenu"><i class="fa fa-reorder"></i></a>

            <div id="sidebar">
                <div id="sidebar-wrapper">
                    <div id="sidebar-inner">
                        <!-- Profile/logo section-->
                        <div id="profile" class="clearfix">
                            <div class="portrate hidden-xs"></div>
                            <div class="title">
                                <h2>Jie Fu</h2>
                                <h3>Université de Montréal / Polytechnique Montréal</h3>
                            </div>   
                        </div>
                        <!-- /Profile/logo section-->

                        <!-- Main navigation-->
                        <div id="main-nav">
                            <ul id="navigation">
                                <li>
                                  <a href="index.html">
                                    <i class="fa fa-user"></i>
                                    <div class="text">About Me</div>
                                  </a>
                                </li>  
                                
                                <li>
                                  <a href="research.html">
                                    <i class="fa fa-bomb"></i>
                                    <div class="text">Research</div>
                                  </a>
                                </li> 
                                
                                <li class="currentmenu">
                                  <a href="publication.html">
                                    <i class="fa fa-edit"></i>
                                    <div class="text">Publications & Code</div>
                                  </a>
                                </li> 

                                <li>
                                  <a href="projects.html">
                                    <i class="fa fa-rocket"></i>
                                    <div class="text">Projects</div>
                                  </a>
                                </li>

                                <li>
                                  <a href="https://www.gitbook.com/@bigaidream/" target="_blank">
                                    <div class="fa fa-book"></div>
                                    <div class="text">GitBooks</div>
                                  </a>
                                </li>  
<!--                                 <li>
                                  <a href="https://bigaidream.gitbooks.io/tech-blog/content/" target="_blank">
                                    <div class="fa fa-microphone"></div>
                                    <div class="text">Tech Blog</div>
                                  </a>
                                </li>    -->
                                <li>
                                  <a href="http://www.jianshu.com/u/40b072692678" target="_blank">
                                    <div class="fa fa-glass"></div>
                                    <div class="text">Life Blog (Chinese)</div>
                                  </a>
                                </li>  
<!--                                 <li>
                                  <a href="gallery.html">
                                    <i class="fa fa-picture-o"></i>
                                    <div class="text">Gallery</div>
                                  </a>
                                </li> -->

                                <li>
                                  <a href="contact.html">
                                      <i class="fa fa-comments"></i>
                                      <div class="text">Contact & Misc</div>
                                  </a>
                                </li>

<!--                                 <li class="external">
                                  <a href="cv.pdf">
                                      <i class="fa fa-download"></i>
                                      <div class="text">Download CV</div>
                                  </a>
                                </li> -->
                            </ul>
                        </div>
                        <!-- /Main navigation-->
                        <!-- Sidebar footer -->
                        <div id="sidebar-footer">
                            <div class="social-icons">
                                <ul>
                                    <li><a href="https://github.com/bigaidream" title="GitHub" target="_blank"><i class="fa fa-github"></i></a></li>
<!--                                     <li><a href="https://www.facebook.com/bigaidream" title="Facebook" target="_blank"><i class="fa fa-facebook"></i></a></li> -->
                                    <li><a href="https://twitter.com/bigaidream" title="Twitter" target="_blank"><i class="fa fa-twitter"></i></a></li>
                                    <li><a href="https://www.linkedin.com/in/bigaidream/" title="Linkedin" target="_blank"><i class="fa fa-linkedin"></i></a></li>
                                    <li><a href="https://www.kaggle.com/bigaidream" title="Kaggle Competition" target="_blank"><i class="fa fa-flask"></i></a></li>
                                    
                                </ul>
                            </div>

        
                            <!-- <div id="copyright">Copyright text here</div> -->
                    
                        </div>
                        <!-- /Sidebar footer -->
                    </div>

                </div>
            </div>

            <div id="main">
            
                <div id="publications" class="page">
                    <div class="page-container">
                        <div class="pageheader">
                            <div class="headercontent">
                                <div class="section-container">
                                    <h2 class="title">Papers...</h2>
                                </div>
                            </div>
                        </div>

                        <div class="pagecontents">
                            
                            <div class="section color-1" id="filters">
                                <div class="section-container">
                                    <div class="row">
                                        
                                        <div class="col-md-3">
                                            <h3>Filter by type:</h3>
                                        </div>
                                        <div class="col-md-6">
                                            <select id="cd-dropdown" name="cd-dropdown" class="cd-select">
                                                <option class="filter" value="all" selected>All types</option>
                                                <option class="filter" value="dl">Deep Learning</option>
                                                <!-- <option class="filter" value="optimization">Optimization</option> -->
                                                <!-- <option class="filter" value="bookchapter">Book Chapters</option> -->
                                                <!-- <option class="filter" value="book">Books</option> -->
                                                <option class="filter" value="others">Others</option>
                                                <!-- <option class="filter" value="tpaper">Technical Papers</option> -->
                                                <!-- <option class="filter" value="thesis">Thesis</option> -->
                                            </select>
                                        </div>
                                        
                                        <div class="col-md-3" id="sort">
                                            <span>Sort by year:</span>
                                            <div class="btn-group pull-right"> 

                                                <button type="button" data-sort="data-year" data-order="desc" class="sort btn btn-default"><i class="fa fa-sort-numeric-asc"></i></button>
                                                <button type="button" data-sort="data-year" data-order="asc" class="sort btn btn-default"><i class="fa fa-sort-numeric-desc"></i></button>
                                            </div>
                                        </div>    
                                    </div>
                                </div>
                            </div>

                            <div class="section color-2" id="pub-grid">
                                <div class="section-container">
                                    
                                    <div class="row">
                                        <div class="col-md-12">
                                            <div class="pitems">



                                                    <div class="item mix dl" data-year="2017">
                                                    <div class="pubmain">
                                                        <div class="pubassets">
                                                            
                                                            <a href="#" class="pubcollapse">
                                                                <i class="fa fa-expand"></i>
                                                            </a>
<!--                                                             <a href="https://github.com/SkTim/bdfa-torch" class="tooltips" title="Source Code" target="_blank">
                                                                <i class="fa fa-github"></i>
                                                            </a> -->
 <!--                                                            <a href="#" target="_blank">
                                                                <i class="fa fa-external-link"></i>
                                                            </a> -->
                                                            <a href="https://7bce9816-a-62cb3a1a-s-sites.googlegroups.com/site/automl2017icml/accepted-papers/AutoML_2017_paper_7.pdf" class="tooltips" title="View Paper" target="_blank">
                                                                <i class="fa fa-file-pdf-o"></i>
                                                            </a>
                                                            
                                                        </div>

                                                        <h4 class="pubtitle">Neural Optimizers with Hypergradients for Tuning Parameter-Wise Learning Rates </h4>
                                                        <div class="pubauthor"><strong>Jie Fu</strong>*, Ritchie Ng*(equal contribution), Danlu Chen, Ilija Ilievski, Christopher Pal, Tat-Seng Chua </div>
                                                        <div class="pubcite"><span class="label label-warning">Deep Learning</span> 2017 ICML AutoML Workshop</div>
                                                        
                                                    </div>
                                                    <div class="pubdetails">
                                                        <!-- <img alt="image" src="img/pubs/DrMAD.jpg" align="left"  style="padding:0 30px 30px 0;"> -->
                                                        <h4>Abstract</h4>
                                                        <p>Recent studies show that LSTM-based neural optimizers are competitive with state-of-the-art hand-designed optimization methods for short horizons. Existing neural optimizers learn how to update the optimizee parameters, namely, predicting the product of learning rates and gradients directly and we suspect it's the reason why the training task becomes unnecessarily difficult. Instead, we train a neural optimizer to only control the learning rates of another optimizer using gradients of the training loss with respect to the learning rates. Furthermore, with the assumption that learning rates tend to remain unchanged over a certain number of iterations, the neural optimizer is only allowed to propose learning rates every $S$ iterations where the learning rates are fixed during these $S$ iterations and this enables it to generalize to longer horizons. The optimizee is trained by Adam on MNIST, and our neural optimizer learns to tune the learning rates for the Adam. After 5 meta-iterations, another optimizee trained by Adam whose learning rates are tuned by the learned but frozen neural optimizer, outperforms those trained by existing hand-designed and learned neural optimizers in terms of convergence rate and final accuracy for long horizons across several datasets.</p>
                                                    </div>
                                                </div>



                                                    <div class="item mix dl" data-year="2017">
                                                    <div class="pubmain">
                                                        <div class="pubassets">
                                                            
                                                            <a href="#" class="pubcollapse">
                                                                <i class="fa fa-expand"></i>
                                                            </a>
                                                            <a href="https://github.com/SkTim/bdfa-torch" class="tooltips" title="Source Code" target="_blank">
                                                                <i class="fa fa-github"></i>
                                                            </a>
 <!--                                                            <a href="#" target="_blank">
                                                                <i class="fa fa-external-link"></i>
                                                            </a> -->
                                                            <a href="https://arxiv.org/abs/1702.07097" class="tooltips" title="View Paper" target="_blank">
                                                                <i class="fa fa-file-pdf-o"></i>
                                                            </a>
                                                            
                                                        </div>

                                                        <h4 class="pubtitle">Bidirectional Backpropagation: Towards Biologically Plausible Error Signal Transmission in Neural Networks </h4>
                                                        <div class="pubauthor">Hongyin Luo*, <strong>Jie Fu</strong>* (equal contribution), James Glass </div>
                                                        <div class="pubcite"><span class="label label-warning">Deep Learning</span> 2017 arXiv preprint</div>
                                                        
                                                    </div>
                                                    <div class="pubdetails">
                                                        <!-- <img alt="image" src="img/pubs/DrMAD.jpg" align="left"  style="padding:0 30px 30px 0;"> -->
                                                        <h4>Abstract</h4>
                                                        <p>The back-propagation (BP) algorithm has been considered the de-facto method for training deep neural networks. It back-propagates errors from the output layer to the hidden layers in an exact manner using the transpose of the feedforward weights. However, it has been argued that this is not biologically plausible because back-propagating error signals with the exact incoming weights is not considered possible in biological neural systems. In this work, we propose a biologically plausible paradigm of neural architecture based on related literature in neuroscience and asymmetric BP-like methods. Specifically, we propose two bidirectional learning algorithms with trainable feedforward and feedback weights. The feedforward weights are used to relay activations from the inputs to target outputs. The feedback weights pass the error signals from the output layer to the hidden layers. Different from other asymmetric BP-like methods, the feedback weights are also plastic in our framework and are trained to approximate the forward activations. Preliminary results show that our models outperform other asymmetric BP-like methods on the MNIST and the CIFAR-10 datasets.</p>
                                                    </div>
                                                </div>





                                                <div class="item mix dl" data-year="2016">
                                                    <div class="pubmain">
                                                        <div class="pubassets">
                                                            
                                                            <a href="#" class="pubcollapse">
                                                                <i class="fa fa-expand"></i>
                                                            </a>
                                                            <a href="https://github.com/bigaidream-projects/drmad" class="tooltips" title="Source Code" target="_blank">
                                                                <i class="fa fa-github"></i>
                                                            </a>
 <!--                                                            <a href="#" target="_blank">
                                                                <i class="fa fa-external-link"></i>
                                                            </a> -->
                                                            <a href="http://arxiv.org/abs/1601.00917" class="tooltips" title="View Paper" target="_blank">
                                                                <i class="fa fa-file-pdf-o"></i>
                                                            </a>
                                                            
                                                        </div>

                                                        <h4 class="pubtitle">DrMAD: Distilling Reverse-Mode Automatic Differentiation for Optimizing Hyperparameters of Deep Neural Networks </h4>
                                                        <div class="pubauthor"><strong>Jie Fu</strong>, Hongyin Luo, Jiashi Feng, Kian Hsiang Low, Tat-Seng Chua</div>
                                                        <div class="pubcite"><span class="label label-warning">Deep Learning</span> IJCAI, 2016, <25% of acceptance</div>
                                                        
                                                    </div>
                                                    <div class="pubdetails">
                                                    	<img alt="image" src="img/pubs/DrMAD.jpg" align="left"  style="padding:0 30px 30px 0;">
                                                        <h4>Abstract</h4>
                                                        <p>The performance of deep neural networks is well-known to be sensitive to the setting of their hyperparameters. Recent advances in reverse-mode automatic differentiation allow for optimizing hyperparameters with gradients. The standard way of computing these gradients involves a forward and backward pass of computations. However, the backward pass usually needs to consume unaffordable memory to store all the intermediate variables to exactly reverse the forward training procedure. In this work we propose a simple but effective method, DrMAD, to distill the knowledge of the forward pass into a shortcut path, through which we approximately reverse the training trajectory. Experiments on several image benchmark datasets show that DrMAD is at least 45 times faster and consumes 100 times less memory compared to state-of-the-art methods for optimizing hyperparameters with minimal compromise to its effectiveness. To the best of our knowledge, DrMAD is the first research attempt to make it practical to automatically tune thousands of hyperparameters of deep neural networks.</p>
                                                    </div>
                                                </div>

                                                
                                                <div class="item mix others" data-year="2015">
                                                    <div class="pubmain">
                                                        <div class="pubassets">
                                                            
                                                            <a href="#" class="pubcollapse">
                                                                <i class="fa fa-expand"></i>
                                                            </a>
 <!--                                                            <a href="#" target="_blank">
                                                                <i class="fa fa-external-link"></i>
                                                            </a> -->
                                                            <a href="http://sentic.net/api/" class="tooltips" title="API" target="_blank">
                                                                <i class="fa fa-github"></i>
                                                            </a>
                                                            <a href="http://sentic.net/affectivespace.pdf" class="tooltips" title="View Paper" target="_blank">
                                                                <i class="fa fa-file-pdf-o"></i>
                                                            </a>
                                                            
                                                        </div>

                                                        <h4 class="pubtitle">AffectiveSpace 2: Enabling Affective Intuition for Concept-Level Sentiment Analysis</h4>
                                                        <div class="pubauthor">Erik Cambria, <strong>Jie Fu</strong>,  Federica Bisio, Soujanya Poria</div>
                                                        <div class="pubcite"><span class="label label-warning">Others</span> AAAI, 2015, 26% of acceptance</div>
                                                        
                                                    </div>
                                                    <div class="pubdetails">
                                                        <h4>Abstract</h4>
                                                        <p>Predicting the affective valence of unknown multiword expressions is key for concept-level sentiment analysis. ActiveSpace 2 is a vector space model, built by means of random projection, that allows for reasoning by analogy on natural language concepts. By reducing the dimensionality of affective common-sense knowledge, the model allows semantic features associated with concepts to be generalized and, hence, allows concepts to be intuitively clustered according to their semantic and affective relatedness. Such an affective intuition (so called because it does not rely on explicit features, but rather on implicit analogies) enables the inference of emotions and polarity conveyed by multi-word expressions, thus achieving effcient concept-level sentiment analysis.</p>
                                                    </div>
                                                </div>



  <!--                                               <div class="item mix dl" data-year="2012">
                                                    <div class="pubmain">
                                                        <div class="pubassets">
                                                            
                                                            <a href="#" class="pubcollapse">
                                                                <i class="fa fa-expand"></i>
                                                            </a>

                                                            <a href="http://otago.ourarchive.ac.nz/bitstream/handle/10523/2360/FuJie2012MSc3.pdf?sequence=3&isAllowed=y" class="tooltips" title="View Paper" target="_blank">
                                                                <i class="fa fa-file-pdf-o"></i>
                                                            </a>
                                                            
                                                        </div>

                                                        <h4 class="pubtitle">Learning Hierarchical Sparse Filters for Feature Matching </h4>
                                                        <div class="pubauthor"><strong>Jie Fu</strong></div>
                                                        <div class="pubcite"><span class="label label-warning">Deep Learning</span> University of Otago, 2012</div>
                                                        
                                                    </div>
                                                    <div class="pubdetails">
                                                    	<img alt="image" src="img/pubs/feature_matching.jpg" align="left"  style="padding:0 30px 30px 0;">
                                                        <h4>Abstract</h4>
                                                        <p>A common problem in computer vision is to match corresponding points between images. The success of computer vision has usually relied on having good feature representations, which are usually hand-crafted and thus require huge amounts of prior knowledge and human labor. To the best of my knowledge, feature learning algorithms have not been used for feature matching in the literature. In this thesis, I will present how to use feature learning algorithms to build useful feature representations from aerial images in a biologically-inspired and unsupervised manner. These learned feature representations can then be used to do feature matching tasks. Specifically, I will present two algorithms, Sparse Filtering and Deconvolutional Networks for generating hierarchical representations, in which the information from the lower levels is grouped to establish complex features. These complex and hierarchical representations often lead to performance approaching highly hand-designed computer vision algorithms in feature matching tasks.</p>
                                                    </div>
                                                </div> -->

                                                <div class="item mix others" data-year="2010">
                                                    <div class="pubmain">
                                                        <div class="pubassets">
                                                            
                                                            <a href="#" class="pubcollapse">
                                                                <i class="fa fa-expand"></i>
                                                            </a>
                                                            <a href="https://github.com/bigaidream/published/blob/master/conferences/IWACI_GPU.m" class="tooltips" title="Source Code" target="_blank">
                                                                <i class="fa fa-github"></i>
                                                            </a>
                                                            <a href="https://github.com/bigaidream/published/blob/master/conferences/aco_air_gpu.pdf" class="tooltips" title="View Paper" target="_blank">
                                                                <i class="fa fa-file-pdf-o"></i>
                                                            </a>
                                                            
                                                        </div>

                                                        <h4 class="pubtitle">A Parallel Ant Colony Optimization Algorithm with GPU-Acceleration Based on All-In-Roulette Selection </h4>
                                                        <div class="pubauthor"><strong>Jie Fu</strong>, Lin Lei, Guohua Zhou</div>
                                                        <div class="pubcite"><span class="label label-warning">Others</span> International Workshop on Advanced Computational Intelligence 2010</div>
                                                        
                                                    </div>
                                                    <div class="pubdetails">
                                                        <h4>Abstract</h4>
                                                        <p>MATLAB source code can be downloaded <a href="https://github.com/bigaidream/published/blob/master/conferences/IWACI_GPU.m" target="_blank">here</a>.</p>
                                                        <p>Ant Colony Optimization is computationallyexpensive when it comes to complex problems. This paper presents andimplements a parallel MAX-MIN Ant System (MMAS) basedon a GPU+CPU hardware platform under the MATLABenvironment with Jacket toolbox to solve Traveling SalesmanProblem (TSP). The key idea is to let all ants share only onepseudorandom number matrix, one pheromone matrix, onetaboo matrix, and one probability matrix. We also use a newselection approach based on those matrices, named AIR (All-In-Roulette). The main contribution of this paper is thedescription of how to design parallel MMAS based on thoseideas and the comparison to the relevant sequential version. Thecomputational results show that our parallel algorithm is muchmore efficient than the sequential version.</p>
                                                    </div>
                                                </div>




                                            </div>
                                        </div>
                                    </div>

                                </div>
                            </div>




                        </div>
                    </div>

                </div>

                
            </div>
        </div>
    </body>
</html>

